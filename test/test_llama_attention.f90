program test_llama_attention_layer
  use iso_fortran_env, only: stderr => error_unit
  use llmf_llama_attention, only: llama_attention_layer
  use llmf_utils, only: allclose, assert_that
  implicit none

  logical :: ok = .true.

  call test_rotary_pos(ok)
  call test_forward_qwen(ok)
  call test_shapes(ok)
  call test_backward_qwen(ok)

  if (.not. ok) then
    write(stderr, '(a)') 'test_llama_attention_layer: one or more tests have failed'
  end if

contains
  subroutine set_weights_qwen(attention)
    type(llama_attention_layer), intent(inout) :: attention
    attention % query_layer % weights = 0.1
    attention % key_layer % weights = 0.2
    attention % value_layer % weights = 0.3
    attention % output_layer % weights = 0.2
    attention % query_layer % biases = 0.11
    attention % key_layer % biases = 0.11
    attention % value_layer % biases = 0.11
  end subroutine set_weights_qwen

  subroutine test_rotary_pos(ok)
    logical, intent(inout) :: ok
    type(llama_attention_layer) :: attention
    real :: q(9, 2, 4) = reshape([&
      -0.04455325, 0.02914775, 0.06434231, 0.08960691, 0.06640552, 0.0849856, -0.04064921, 0.00200219, -0.02962775,&
      -0.09094961, -0.06784477, 0.02359047, -0.00346221, -0.00215055, 0.07274084, 0.06654175, 0.05239149, -0.09460114,&
      0.10063736, 0.07616261, -0.04158672, -0.03353539, 0.03262881, -0.06504188, -0.10759991, -0.11058792, 0.07655542,&
      -0.10021649, 0.15178838, -0.05707106, 0.16878726, 0.0294596, -0.02350482, -0.11376162, 0.01817995, -0.00764011,&
      -0.0588729, -0.00190191, 0.06911292, 0.06933947, 0.03334169, 0.06592909, 0.01265682, 0.04764761, -0.04075027,&
      -0.02562499, -0.02326545, -0.0622794, -0.00586975, -0.01146702, -0.00159615, 0.04314973, 0.04185499, -0.02979277,&
      -0.04437296, -0.02111869, 0.04181552, 0.01909114, -0.03078587, 0.02573479, 0.0235435, 0.03370059, -0.02030428,&
      0.07409419, 0.06405655, 0.08842005, 0.04825712, 0.09055403, 0.01358884, -0.06541353, -0.04184227, 0.05136994&
    ], [9, 2, 4])
    real :: k(9, 2, 2) = reshape([&
      -0.03239182, 0.03906113, -0.03085477, -0.0307175, 0.00146978, -0.05166513, -0.06575582, 0.07144419, -0.01427399,&
      -0.02034676, -0.0089988, -0.04418612, -0.00677515, -0.09799442, -0.04022231, -0.0011375, 0.04453745, 0.03444292,&
      -0.00334935, 0.03490792, 0.01827038, 0.05009681, -0.00683554, -0.00564786, -0.0307076, 0.02890428, 0.03051437,&
      -0.03344896, 0.0454146, 0.06500776, 0.04522298, 0.1468908, 0.05248369, -0.0234762, 0.04694936, -0.0827937&
    ], [9, 2, 2])
    real :: cosine(9, 2) = reshape([&
      1., 0.54030234, -0.41614684, -0.9899925, -0.6536436, 0.2836622, 0.96017027, 0.75390226, -0.14550003,&
      1., 0.54030234, -0.41614684, -0.9899925, -0.6536436, 0.2836622, 0.96017027, 0.75390226, -0.14550003&
    ], [9, 2])
    real :: sine(9, 2) = reshape([&
      0., 0.84147096, 0.9092974, 0.14112, -0.7568025, -0.9589243, -0.2794155, 0.6569866, 0.98935825,&
      0., 0.84147096, 0.9092974, 0.14112, -0.7568025, -0.9589243, -0.2794155, 0.6569866, 0.98935825&
    ], [9, 2])
    real :: expected_query(9, 2, 4) = reshape([&
      -0.04455325, 0.072838, -0.0482266, -0.08822158, -0.04503309, 0.09386016, -0.02043736, -0.03291105, 0.09790526,&
      -0.09094961, -0.0121297, 0.04868919, 0.01607289, -0.04885017, -0.06086093, 0.07524943, 0.04081348, -0.01554799,&
      0.10063736, -0.08657468, 0.06920075, 0.00938053, 0.00096748, -0.04098927, -0.135101, -0.09531647, -0.00358001,&
      -0.10021649, 0.14610024, -0.01406476, -0.17183063, -0.04394965, 0.05570281, -0.07916544, -0.05894888, 0.07685237,&
      -0.0588729, 0.01854959, 0.02786938, -0.06781722, -0.03047185, 0.017171, 0.02420941, 0.00842347, 0.03540489,&
      -0.02562499, -0.01417078, 0.08876158, 0.01559619, -0.01773773, -0.06367377, 0.03789458, 0.06285841, -0.03598176,&
      -0.04437296, -0.0653122, -0.09780152, -0.02571013, 0.0886545, 0.02033066, 0.00432822, 0.05289676, -0.047869,&
      0.07409419, 0.01683914, 0.00122702, -0.04508005, -0.03589124, -0.02082307, -0.06938655, -0.00940414, -0.02756254&
    ], [9, 2, 4])
    real :: expected_key(9, 2, 2) = reshape([&
      -0.03239182, 0.02867705, 0.05301844, 0.03136621, -0.07512313, -0.0532256, -0.06345462, 0.02460143, -0.03199953,&
      -0.02034676, 0.02800674, -0.00966824, 0.00237249, 0.06294109, 0.0381334, 0.017281, 0.08051476, -0.01913353,&
      -0.00334935, -0.01935424, -0.06671456, -0.05597733, 0.11563534, 0.04872581, -0.03604414, -0.00905411, 0.07747279,&
      -0.03344896, 0.05391162, -0.01043957, -0.03770074, -0.09084108, 0.02030351, -0.01396097, 0.05438495, 0.04223613&
    ], [9, 2, 2])

    attention = llama_attention_layer(n_heads=4, n_kv_heads=2)
    call attention % init([9, 8])

    call attention % apply_rotary_pos_emb(q, k, cosine, sine)

    call assert_that(allclose(q, expected_query), ok, 'incorrect rotary positional encoding for query')
    call assert_that(allclose(k, expected_key), ok, 'incorrect rotary positional encoding for key')
  end subroutine test_rotary_pos

  subroutine test_forward_qwen(ok)
    logical, intent(inout) :: ok
    type(llama_attention_layer) :: attention
    real :: input(9, 8) = reshape([&
      0.37072068, 1.3588632, 1.9884692, -0.30161408, 0.8139249, -0.8694511, -1.6688265, 1.1662576,&
      0.2468737, -0.9110192, 0.04235176, -0.45361742, 0.36150697, 0.8447159, 0.5774916, 0.08439375,&
      -0.6989709, -1.0674919, -1.1826763, -0.880081, 1.0227386, 0.2537887, 1.6236242, 1.6021129,&
      1.6466041, 1.0528897, -2.2763023, 1.2870983, 0.12475284, 0.13161187, -1.5640669, -0.14536288,&
      -0.5928059, -0.6204104, -1.8023925, 0.3605392, -0.47602218, -1.2931924, 1.0930126, -0.30460525,&
      -0.72116065, 1.3226725, -0.40716624, 0.67106473, 0.33057117, -0.2370891, 0.00500101, 1.2334017,&
      1.8437266, 0.14475724, 1.4683013, -0.17329404, -1.1277869, 0.5619295, 1.1053102, 1.0893365,&
      -0.13279338, 0.41584253, 1.8390387, -0.24315795, -0.71289355, 0.00285761, 0.29787052,&
      -1.5655739, 1.5732917, -0.17363702, 1.2494951, 0.10759168, -0.08730965, -1.1767657, -0.23054339, -0.97817636&
    ], [9, 8])
    real :: cosine(9, 2) = reshape([&
      1., 0.54030234, -0.41614684, -0.9899925, -0.6536436, 0.2836622, 0.96017027, 0.75390226, -0.14550003,&
      1., 0.54030234, -0.41614684, -0.9899925, -0.6536436, 0.2836622, 0.96017027, 0.75390226, -0.14550003&
    ], [9, 2])
    real :: sine(9, 2) = reshape([&
      0., 0.84147096, 0.9092974, 0.14112, -0.7568025, -0.9589243, -0.2794155, 0.6569866, 0.98935825,&
      0., 0.84147096, 0.9092974, 0.14112, -0.7568025, -0.9589243, -0.2794155, 0.6569866, 0.98935825&
    ], [9, 2])
    real :: causal_mask(9, 9)  ! generation code below
    real :: expected_no_mask_output(9, 8) = reshape([&
      0.621131539, 0.637778699, 0.767387390, 0.655714452, 0.710335255, 0.631989479, 0.640926480, 0.614895999,&
      0.612184584, 0.621131539, 0.637778699, 0.767387390, 0.655714452, 0.710335255, 0.631989479, 0.640926480,&
      0.614895999, 0.612184584, 0.621131539, 0.637778699, 0.767387390, 0.655714452, 0.710335255, 0.631989479,&
      0.640926480, 0.614895999, 0.612184584, 0.621131539, 0.637778699, 0.767387390, 0.655714452, 0.710335255,&
      0.631989479, 0.640926480, 0.614895999, 0.612184584, 0.621131539, 0.637778699, 0.767387390, 0.655714452,&
      0.710335255, 0.631989479, 0.640926480, 0.614895999, 0.612184584, 0.621131539, 0.637778699, 0.767387390,&
      0.655714452, 0.710335255, 0.631989479, 0.640926480, 0.614895999, 0.612184584, 0.621131539, 0.637778699,&
      0.767387390, 0.655714452, 0.710335255, 0.631989479, 0.640926480, 0.614895999, 0.612184584, 0.621131539,&
      0.637778699, 0.767387390, 0.655714452, 0.710335255, 0.631989479, 0.640926480, 0.614895999, 0.612184584&
    ], [9, 8])
    real :: expected_causal_mask_output(9, 8) = reshape([&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847,&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847,&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847,&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847,&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847,&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847,&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847,&
      -0.5964408, 0.39469138, 1.3986229, 1.0801082, 1.412577, 1.3915275, 0.97864, 0.8210332, 0.6121847&
    ], [9, 8])
    integer :: i, j

    ! causal attention mask, e.g.:
    ! [0   -100. -100.]
    ! [0   0.    -100.]
    ! [0   0.    0.   ]
    causal_mask = 0.
    forall(i = 1: 9, j = 1: 9, i < j) causal_mask(i, j) = -100.

    attention = llama_attention_layer(n_heads=4, n_kv_heads=2, is_qwen=.true.)
    call attention % init([9, 8])
    call set_weights_qwen(attention)

    call attention % forward(input, cosine, sine)
    call assert_that(allclose(attention % output, expected_no_mask_output), ok, 'incorrect forward output (no mask)')

    call attention % forward(input, cosine, sine, causal_mask)
    call assert_that(&
        allclose(attention % output, expected_causal_mask_output), ok, 'incorrect forward output (causal mask)'&
    )
  end subroutine test_forward_qwen

  subroutine test_shapes(ok)
    logical, intent(inout) :: ok
    type(llama_attention_layer) :: attention

    real :: input(9, 896)
    real :: cos_(9, 64)
    real :: sin_(9, 64)

    call random_number(input)
    call random_number(sin_)
    call random_number(cos_)

    attention = llama_attention_layer(n_heads=14, n_kv_heads=2)
    call attention % init([9, 896])

    call assert_that(all(shape(attention % q_temp) == [9, 64, 14]), ok, 'incorrect query projection shape')
    call assert_that(all(shape(attention % k_temp) == [9, 64, 2]), ok, 'incorrect key projection shape')
    call assert_that(all(shape(attention % v_temp) == [9, 64, 2]), ok, 'incorrect value projection shape')
    call assert_that(&
        all(shape(attention % q_or_dq) == [9, 64, 14]) &
          .and. all(shape(attention % k_or_dk) == [9, 64, 14]) &
          .and. all(shape(attention % v_or_dv) == [9, 64, 14]), &
        ok, 'sdpa inputs have inconsistent dimensions'&
    )

    call attention % forward(input, cos_, sin_)
    call assert_that(&
        all(shape(attention % output) == shape(input)), ok, 'inconsistent shape between attention input and output'&
    )
  end subroutine test_shapes

  subroutine test_backward_qwen(ok)
    logical, intent(inout) :: ok
    type(llama_attention_layer) :: attention
    real :: input(9, 8) = reshape([&
      0.37072068, 1.3588632, 1.9884692, -0.30161408, 0.8139249, -0.8694511, -1.6688265, 1.1662576,&
      0.2468737, -0.9110192, 0.04235176, -0.45361742, 0.36150697, 0.8447159, 0.5774916, 0.08439375,&
      -0.6989709, -1.0674919, -1.1826763, -0.880081, 1.0227386, 0.2537887, 1.6236242, 1.6021129,&
      1.6466041, 1.0528897, -2.2763023, 1.2870983, 0.12475284, 0.13161187, -1.5640669, -0.14536288,&
      -0.5928059, -0.6204104, -1.8023925, 0.3605392, -0.47602218, -1.2931924, 1.0930126, -0.30460525,&
      -0.72116065, 1.3226725, -0.40716624, 0.67106473, 0.33057117, -0.2370891, 0.00500101, 1.2334017,&
      1.8437266, 0.14475724, 1.4683013, -0.17329404, -1.1277869, 0.5619295, 1.1053102, 1.0893365,&
      -0.13279338, 0.41584253, 1.8390387, -0.24315795, -0.71289355, 0.00285761, 0.29787052,&
      -1.5655739, 1.5732917, -0.17363702, 1.2494951, 0.10759168, -0.08730965, -1.1767657, -0.23054339, -0.97817636&
    ], [9, 8])
    real :: cosine(9, 2) = reshape([&
      1., 0.54030234, -0.41614684, -0.9899925, -0.6536436, 0.2836622, 0.96017027, 0.75390226, -0.14550003,&
      1., 0.54030234, -0.41614684, -0.9899925, -0.6536436, 0.2836622, 0.96017027, 0.75390226, -0.14550003&
    ], [9, 2])
    real :: sine(9, 2) = reshape([&
      0., 0.84147096, 0.9092974, 0.14112, -0.7568025, -0.9589243, -0.2794155, 0.6569866, 0.98935825,&
      0., 0.84147096, 0.9092974, 0.14112, -0.7568025, -0.9589243, -0.2794155, 0.6569866, 0.98935825&
    ], [9, 2])
    real :: causal_mask(9, 9)
    integer :: i, j
    real :: gradient(9, 8) = reshape([&
      0.2643, 0.5053, 0.8736, 0.7107, 0.4371, 0.7562, 0.4131, 0.2426, 0.3533,&
      0.4271, 0.8181, 0.9274, 0.3118, 0.4004, 0.7692, 0.8253, 0.8845, 0.5925,&
      0.8704, 0.5688, 0.6746, 0.9214, 0.3114, 0.9759, 0.6638, 0.0278, 0.3023,&
      0.7629, 0.1311, 0.7574, 0.9603, 0.6012, 0.3955, 0.476, 0.9791, 0.9799,&
      0.5051, 0.2189, 0.426, 0.431, 0.3824, 0.7647, 0.7132, 0.1745, 0.3256,&
      0.6396, 0.1247, 0.9805, 0.7546, 0.5645, 0.9313, 0.4169, 0.7757, 0.0203,&
      0.5826, 0.6509, 0.1613, 0.426, 0.8945, 0.3592, 0.5082, 0.8625, 0.3525,&
      0.76, 0.9715, 0.5208, 0.979, 0.1589, 0.6575, 0.1034, 0.0686, 0.5525&
    ], [9, 8])
    real :: expected_gradient(9, 8) = reshape([&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194,&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194,&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194,&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194,&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194,&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194,&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194,&
      6.2219434, 3.8212545, 4.0379825, 2.3054194, 2.4747465, 1.5329164, 0.7791914, 0.40893596, 0.21961194&
    ], [9, 8])

    causal_mask = 0.
    forall(i = 1: 9, j = 1: 9, i < j) causal_mask(i, j) = -100.

    attention = llama_attention_layer(n_heads=4, n_kv_heads=2, is_qwen=.true.)
    call attention % init([9, 8])
    call set_weights_qwen(attention)

    call attention % forward(input, cosine, sine, causal_mask)
    call attention % backward(input, gradient, cosine, sine, causal_mask)

    ! Python Reference: https://github.com/OneAdder/neural-fortran-references/blob/main/llama_attention.py
!    print *, attention % gradient
    ! FIXME: does not apply backward for ropes, so output is slightly off and tests don't pass
    call assert_that(allclose(attention % gradient, expected_gradient), ok, 'incorrect gradient after backward pass')
  end subroutine test_backward_qwen
end program test_llama_attention_layer
